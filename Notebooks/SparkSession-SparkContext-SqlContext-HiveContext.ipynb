{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2343ce",
   "metadata": {},
   "source": [
    "# Relembrando a arquitetura\n",
    "Um aplicativo Spark consiste em um programa de driver e um grupo de executores no cluster.  \n",
    "O Driver é um processo que executa o programa principal de seu aplicativo Spark e cria o SparkContext que coordena a execução de jobs (mais sobre isso mais tarde).  \n",
    "Os executores são processos em execução nos nós de trabalho do cluster que são responsáveis por executar as tarefas que o processo do driver atribuiu a eles.  \n",
    "![image](images/arch.png)\n",
    "\n",
    "O gerenciador de cluster (como Mesos, Kubernetes ou YARN) é responsável pela alocação de recursos físicos para aplicativos Spark.  \n",
    "\n",
    "Cada aplicação Spark precisa de um ponto de entrada que permite que ele se comunique com as fontes de dados e execute certas operações, como ler e gravar dados.  \n",
    "No Spark 1.x, três pontos de entrada foram introduzidos: **SparkContext, SQLContext e HiveContext**.  \n",
    "Desde o Spark 2.x, um novo ponto de entrada chamado **SparkSession** foi introduzido, combinando essencialmente todas as funcionalidades disponíveis nos três contextos mencionados.  \n",
    "\n",
    "*Observe que todos os contextos ainda estão disponíveis mesmo nas versões mais recentes do Spark, principalmente para fins de compatibilidade com versões anteriores.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64d101",
   "metadata": {},
   "source": [
    "# SparkContext\n",
    "\n",
    "O SparkContext é usado pelo Processo de Driver do Aplicativo Spark para estabelecer uma comunicação com o cluster e os gerenciadores de recursos para coordenar e executar jobs. SparkContext também permite o acesso a outros dois contextos, ou seja, SQLContext e HiveContext (mais sobre esses pontos de entrada mais adiante).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96573bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('app') \\\n",
    "    .setMaster(master)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155849f6",
   "metadata": {},
   "source": [
    "# SQLContext\n",
    "SQLContext é o ponto de entrada para SparkSQL, que é um módulo Spark para processamento de dados estruturados. Depois que o SQLContext é inicializado, o usuário pode usá-lo para realizar várias operações \"semelhantes a sql\" em datasets e dataframes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('app') \\\n",
    "    .setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd240b2",
   "metadata": {},
   "source": [
    "# HiveContext\n",
    "Se o seu aplicativo Spark precisa se comunicar com o Hive e você está usando o Spark <2.0, provavelmente precisará de um HiveContext se. Para Spark 1.5+, o HiveContext também oferece suporte para funções de janela.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b101a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, HiveContext\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('app') \\\n",
    "    .setMaster(master)\n",
    "sc = SparkContext(conf)\n",
    "hive_context = HiveContext(sc)\n",
    "hive_context.sql(\"select * from tableName limit 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d096c",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "Spark 2.0 introduziu um novo ponto de entrada chamado SparkSession que substituiu essencialmente SQLContext e HiveContext. \n",
    "Além disso, dá aos desenvolvedores acesso imediato ao SparkContext.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession \\\n",
    "    .builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "# Two ways you can access spark context from spark session\n",
    "spark_context = spark_session._sc\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e592d4e",
   "metadata": {},
   "source": [
    "# Referência\n",
    "\n",
    "https://towardsdatascience.com/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44fd99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
